{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/visha/Documents/GitHub/Club_Mahindra_Data_Hack/Club_Mahindra_Data_Hack')\n",
    "\n",
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "color = sns.color_palette()\n",
    "\n",
    "# Import local libraries\n",
    "from src.models import train_models\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = pd.read_csv(r'../data/processed/preprocessed_data/train_preprocessed.csv')\n",
    "val_preprocessed = pd.read_csv(r'../data/processed/preprocessed_data/val_preprocessed.csv')\n",
    "test_preprocessed = pd.read_csv(r'../data/processed/preprocessed_data/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(r'../data/processed/target_values/train_target_values.csv', header=None)\n",
    "val_target = pd.read_csv(r'../data/processed/target_values/val_target_values.csv', header=None)\n",
    "test_target = pd.read_csv(r'../data/processed/target_values/test_target_values.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Models On Training Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed: 37.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1515561433586956\n"
     ]
    }
   ],
   "source": [
    "adb_reg, train_adb_score = train_models.train_adb(train_preprocessed,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed: 13.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.024060889811498\n"
     ]
    }
   ],
   "source": [
    "etr_reg, train_etr_score = train_models.train_etr(train_preprocessed,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999955347929\n"
     ]
    }
   ],
   "source": [
    "xgb_reg, train_xgb_score = train_models.train_xgb(train_preprocessed,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the models to disk\n",
    "adb_filename = r'../models/adaboost_model.sav'\n",
    "etr_filename = r'../models/extratrees_model.sav'\n",
    "xgb_filename = r'../models/xgboost_model.sav'\n",
    "\n",
    "pickle.dump(adb_reg, open(adb_filename, 'wb'))\n",
    "pickle.dump(etr_reg, open(etr_filename, 'wb'))\n",
    "pickle.dump(xgb_reg, open(xgb_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack and Blend Models\n",
    "Stack the above 3 models and blend them using a Random Forest Regressor. Then train the blender on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.091667081256894\n"
     ]
    }
   ],
   "source": [
    "blender_model, blender_score = train_models.stack_models(val_preprocessed, val_target, xgb_reg, adb_reg, etr_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate The Best Model On Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model_Name                                              Model  \\\n",
      "0      XGBoost Regressor  XGBRegressor(base_score=0.5, booster='gbtree',...   \n",
      "1     AdaBoost Regressor  (DecisionTreeRegressor(criterion='mse', max_de...   \n",
      "2  Extra Trees Regressor  (ExtraTreeRegressor(criterion='mse', max_depth...   \n",
      "3             RF Blender  RandomForestRegressor(bootstrap=True, criterio...   \n",
      "\n",
      "      Score  \n",
      "0  0.999996  \n",
      "1  1.151556  \n",
      "2  1.024061  \n",
      "3  1.091667  \n",
      "XGBoost Regressor model performs the best on Validation Set\n"
     ]
    }
   ],
   "source": [
    "# Create a table of all the models with their mean cross-validated scores\n",
    "scoreboard = pd.DataFrame({'Model_Name': ['XGBoost Regressor', 'AdaBoost Regressor', 'Extra Trees Regressor', 'RF Blender'],\n",
    "                           'Model': [xgb_reg, adb_reg, etr_reg, blender_model],\n",
    "                           'Score': [train_xgb_score, train_adb_score, train_etr_score, blender_score]})\n",
    "\n",
    "winner_model_name = scoreboard.loc[scoreboard[['Score']].idxmin()]['Model_Name']\n",
    "winner_model = scoreboard.loc[scoreboard[['Score']].idxmin()]['Model']\n",
    "\n",
    "print (scoreboard)\n",
    "print('{} model performs the best on Validation Set'.format(winner_model_name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Root of Winner Model on Test set\n",
    "final_score = train_models.evaluate_on_test(winner_model[0], test_preprocessed, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final Root Mean Squared Error (RMSE score) of XGBoost Regressor on unseen data is 1.34\n"
     ]
    }
   ],
   "source": [
    "print('The final Root Mean Squared Error (RMSE score) of {} on unseen data is {}'.format(winner_model_name[0], np.round(final_score, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Here, I compared three ensemble algorithms, viz. XGBoost, AdaBoost and Extremely Randomized Trees. I also stacked these models and blended them using a Random Forest Regressor. Though XGBoost and Extra Trees Regressors have comparable performances, XGBoost performed better than all others.\n",
    "\n",
    "Also, these models have been trained on a subset of the original data (size of the dataset is 60k rows, considering the processing/ memory capabilities of my machine). If we train the models on a larger size of original data or the entire original data then the performance of these models would certainly improve.\n",
    "\n",
    "I will update the models and their scores once I get train it on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cmdh)",
   "language": "python",
   "name": "club_mahindra_data_hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
